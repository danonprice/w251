{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 . How long does it take to complete the training run? (hint: this session is on distributed training, so it will take a while)\n",
    "\n",
    "> Training took 23h 11m 23s. Training started on Saturday February 29 at 17:36:56. It completed on Sunday March 1 at 16:48:19.\n",
    "\n",
    "2 . Do you think your model is fully trained? How can you tell?\n",
    "\n",
    "> The model is not fully trained after 50k steps. The loss after 50k steps is 1.617. The loss on 300k steps according to the plot on the assignment README suggests the loss dips below 1.6 when trained for a longer time. \n",
    "\n",
    "![](eval_loss.png)\n",
    "\n",
    "3 . Were you overfitting?\n",
    "\n",
    "> The model did not overfit. The learning rate did not flatten in the 50,000 steps. The learning could have decreased more with more training. Additionally, the BLEU score continues to rise and similarly could have risen more with more training.\n",
    "\n",
    "![](bleu_score.png)\n",
    "\n",
    "4 . Were your GPUs fully utilized?\n",
    "\n",
    ">\n",
    "\n",
    "5 . Did you monitor network traffic (hint: apt install nmon ) ? Was network the bottleneck?\n",
    "\n",
    ">\n",
    "\n",
    "6 . Take a look at the plot of the learning rate and then check the config file. Can you explan this setting?\n",
    "\n",
    "![](learning_rate.png)\n",
    "\n",
    "> The config file has \"learning_rate\": 2.0. This initially seems odd in relation to the plot.  The setting that follows, \"warmup_steps\": 8000, helps to explain the setting. The learning rate grows linearly for 8000 steps as a function of the learning_rate setting.\n",
    "\n",
    "7 . How big was your training set (mb)? How many training lines did it contain?\n",
    "\n",
    "> The English training data is over 636 MB. The German training data is over 710 MB. The total training set is 1346 MB. Each of the training files contains 4,562,102 lines.\n",
    "\n",
    "8 . What are the files that a TF checkpoint is comprised of?\n",
    "\n",
    "> The TF checkpoint is comprised of three files. The weights are stored in a .data file in binary format. A .index file indicates where weights are stored. A .meta file describes the network graph structure.\n",
    "\n",
    "9 . How big is your resulting model checkpoint (mb)?\n",
    "\n",
    "> The 3 files that comprise the model checkpoint sum to about 869 MB.\n",
    "\n",
    ">root@v100a:/data/en-de-transformer/best_models# ls -ltr val_loss=1.6172-step-50000.* <br>\n",
    "-rw-r--r-- 1 root root     36131 Mar  2 00:51 val_loss=1.6172-step-50000.index <br>\n",
    "-rw-r--r-- 1 root root 852267044 Mar  2 00:51 val_loss=1.6172-step-50000.data-00000-of-00001 <br>\n",
    "-rw-r--r-- 1 root root  16360030 Mar  2 00:51 val_loss=1.6172-step-50000.meta <br>\n",
    "\n",
    "10 . Remember the definition of a \"step\". How long did an average step take?\n",
    "\n",
    "> The average step took 1.673 seconds according to data written at the conclusion of training. Given 50,000 training steps, this value is consistent with the 23+ hour training time.\n",
    "\n",
    "11 . How does that correlate with the observed network utilization between nodes?\n",
    "\n",
    ">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
